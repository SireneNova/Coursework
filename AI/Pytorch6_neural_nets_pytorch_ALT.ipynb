{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch6-neural-nets-pytorch-ALT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SireneNova/Coursework/blob/master/AI/Pytorch6_neural_nets_pytorch_ALT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d0f528ff-3691-4858-ceb8-5da3f8c19292",
        "id": "nxIVmXmjYEdt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "#https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn, optim\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(), \n",
        "                                transforms.Normalize((.5,), (.5,)),\n",
        "                                ])\n",
        "trainset = datasets.MNIST('PATH_TO_STORE_TRAINSET', download=True, train=True, transform=transform)\n",
        "valset = datasets.MNIST('PATH_TO_STORE_TESTSET', download=True, train=False, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "# plt.imshow(images[0].numpy().squeeze(), cmap='gray_r');\n",
        "\n",
        "# figure = plt.figure()\n",
        "# num_of_images = 60\n",
        "# for index in range(1, num_of_images + 1):\n",
        "#     plt.subplot(6, 10, index)\n",
        "#     plt.axis('off')\n",
        "#     plt.imshow(images[index].numpy().squeeze(), cmap='gray_r')\n",
        "\n",
        "input_size = 784\n",
        "hidden_sizes = [128, 64]\n",
        "output_size = 10\n",
        "\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "print(model)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "images, labels = next(iter(trainloader))\n",
        "images = images.view(images.shape[0], -1)\n",
        "\n",
        "logps = model(images) #log probabilities\n",
        "loss = criterion(logps, labels) #calculate the NLL loss\n",
        "\n",
        "print('Before backward pass: \\n', model[0].weight.grad)\n",
        "loss.backward()\n",
        "print('After backward pass: \\n', model[0].weight.grad)\n",
        "\n",
        "# # sigmoid activation function (probability)\n",
        "# def activation(x):\n",
        "#   return 1/(1+torch.exp(-x))\n",
        "\n",
        "# #flatten the input image\n",
        "# inputs = images.view(images.shape[0], 784);\n",
        "# #inputs = images.view(images.shape[0], 784); #this also works if you don't want to figure out what the 2nd dimension needs to be. automatically matches up.\n",
        "\n",
        "\n",
        "# # Download and load the training data\n",
        "# trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# dataiter = iter(trainloader)\n",
        "\n",
        "# images, labels = dataiter.next()\n",
        "# print(type(images))\n",
        "# print(images.shape)\n",
        "# print(labels.shape)\n",
        "    \n",
        "    \n",
        "# plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');\n",
        "\n",
        "# #flatten the input images (keep the batches the same, but change the dimensions to 1d)\n",
        "# inputs = images.view(images.shape[0], -1) #could put 784, but -1 is shortcut (64 rows, 784 columns?)\n",
        "\n",
        "# #create parameters\n",
        "# w1 = torch.randn(784, 256)\n",
        "# b1 = torch.randn(256)\n",
        "\n",
        "# w2 = torch.randn(256, 10)\n",
        "# b2 = torch.randn(10)\n",
        "\n",
        "# h= activation(torch.mm(inputs, w1) +b1)\n",
        "\n",
        "# out = torch.mm(h, w2) + b2\n",
        "\n",
        "# print(out)\n",
        "# print(out.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n",
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
            "  (5): LogSoftmax()\n",
            ")\n",
            "Before backward pass: \n",
            " None\n",
            "After backward pass: \n",
            " tensor([[ 2.9597e-03,  2.9597e-03,  2.9597e-03,  ...,  2.9597e-03,\n",
            "          2.9597e-03,  2.9597e-03],\n",
            "        [ 5.5080e-03,  5.5080e-03,  5.5080e-03,  ...,  5.5080e-03,\n",
            "          5.5080e-03,  5.5080e-03],\n",
            "        [-2.1852e-03, -2.1852e-03, -2.1852e-03,  ..., -2.1852e-03,\n",
            "         -2.1852e-03, -2.1852e-03],\n",
            "        ...,\n",
            "        [ 3.8373e-03,  3.8373e-03,  3.8373e-03,  ...,  3.8373e-03,\n",
            "          3.8373e-03,  3.8373e-03],\n",
            "        [-3.9074e-05, -3.9074e-05, -3.9074e-05,  ..., -3.9074e-05,\n",
            "         -3.9074e-05, -3.9074e-05],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}